{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Characters: ', 179182)\n",
      "('Total Vocab: ', 55)\n",
      "('Total Patterns: ', 178926)\n",
      "Epoch 1/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 2.5432Epoch 00001: loss improved from 3.02836 to 2.54322, saving model to weights-improvement-01-2.5432-bigger.hdf5\n",
      "178926/178926 [==============================] - 2291s - loss: 2.5432  \n",
      "Epoch 3/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 2.4302Epoch 00002: loss improved from 2.54322 to 2.43022, saving model to weights-improvement-02-2.4302-bigger.hdf5\n",
      "178926/178926 [==============================] - 2371s - loss: 2.4302  \n",
      "Epoch 4/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.4295Epoch 00029: loss improved from 1.45244 to 1.42946, saving model to weights-improvement-29-1.4295-bigger.hdf5\n",
      "178926/178926 [==============================] - 2211s - loss: 1.4295  \n",
      "Epoch 31/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.3919Epoch 00031: loss improved from 1.41240 to 1.39191, saving model to weights-improvement-31-1.3919-bigger.hdf5\n",
      "178926/178926 [==============================] - 2283s - loss: 1.3919  \n",
      "Epoch 33/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.3799Epoch 00032: loss improved from 1.39191 to 1.37989, saving model to weights-improvement-32-1.3799-bigger.hdf5\n",
      "178926/178926 [==============================] - 2265s - loss: 1.3799  \n",
      "Epoch 34/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.3620Epoch 00033: loss improved from 1.37989 to 1.36196, saving model to weights-improvement-33-1.3620-bigger.hdf5\n",
      "178926/178926 [==============================] - 2283s - loss: 1.3620  \n",
      "Epoch 35/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.3430Epoch 00034: loss improved from 1.36196 to 1.34306, saving model to weights-improvement-34-1.3431-bigger.hdf5\n",
      "178926/178926 [==============================] - 2260s - loss: 1.3431  \n",
      "Epoch 36/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.3238Epoch 00035: loss improved from 1.34306 to 1.32392, saving model to weights-improvement-35-1.3239-bigger.hdf5\n",
      "178926/178926 [==============================] - 2265s - loss: 1.3239  \n",
      "Epoch 37/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.3096Epoch 00036: loss improved from 1.32392 to 1.30970, saving model to weights-improvement-36-1.3097-bigger.hdf5\n",
      "178926/178926 [==============================] - 2235s - loss: 1.3097  \n",
      "Epoch 38/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.2964Epoch 00037: loss improved from 1.30970 to 1.29641, saving model to weights-improvement-37-1.2964-bigger.hdf5\n",
      "178926/178926 [==============================] - 2332s - loss: 1.2964  \n",
      "Epoch 39/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.2798Epoch 00038: loss improved from 1.29641 to 1.27990, saving model to weights-improvement-38-1.2799-bigger.hdf5\n",
      "178926/178926 [==============================] - 2222s - loss: 1.2799  \n",
      "Epoch 40/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.2625Epoch 00039: loss improved from 1.27990 to 1.26244, saving model to weights-improvement-39-1.2624-bigger.hdf5\n",
      "178926/178926 [==============================] - 2253s - loss: 1.2624  \n",
      "Epoch 41/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.2455Epoch 00040: loss improved from 1.26244 to 1.24551, saving model to weights-improvement-40-1.2455-bigger.hdf5\n",
      "178926/178926 [==============================] - 2211s - loss: 1.2455  \n",
      "Epoch 42/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.2293Epoch 00041: loss improved from 1.24551 to 1.22931, saving model to weights-improvement-41-1.2293-bigger.hdf5\n",
      "178926/178926 [==============================] - 2247s - loss: 1.2293  \n",
      "Epoch 43/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.2166Epoch 00042: loss improved from 1.22931 to 1.21666, saving model to weights-improvement-42-1.2167-bigger.hdf5\n",
      "178926/178926 [==============================] - 2253s - loss: 1.2167  \n",
      "Epoch 44/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 1.2039Epoch 00043: loss improved from 1.21666 to 1.20388, saving model to weights-improvement-43-1.2039-bigger.hdf5\n",
      "178926/178926 [==============================] - 2278s - loss: 1.2039  \n",
      "Epoch 45/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 0.9461Epoch 00066: loss improved from 0.95641 to 0.94596, saving model to weights-improvement-66-0.9460-bigger.hdf5\n",
      "178926/178926 [==============================] - 2268s - loss: 0.9460  \n",
      "Epoch 68/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 0.9411Epoch 00067: loss improved from 0.94596 to 0.94115, saving model to weights-improvement-67-0.9412-bigger.hdf5\n",
      "178926/178926 [==============================] - 2268s - loss: 0.9412  \n",
      "Epoch 69/5000\n",
      "178880/178926 [============================>.] - ETA: 0s - loss: 0.9227Epoch 00069: loss improved from 0.92951 to 0.92270, saving model to weights-improvement-69-0.9227-bigger.hdf5\n",
      "178926/178926 [==============================] - 2317s - loss: 0.9227  \n",
      "Epoch 71/5000\n",
      " 65280/178926 [=========>....................] - ETA: 1447s - loss: 0.8532"
     ]
    }
   ],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "# 88 percent accuracy\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\" # Will use only 2nd, 3rd and 4th gpu\n",
    "\n",
    "\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"eminescu_no_spaces.csv\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 256\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(1024))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(output_dim = 1024, activation = 'relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheck\n",
    "\n",
    "\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, nb_epoch=5000, batch_size=64, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}