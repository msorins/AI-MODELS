{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = \"tensorflow\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINES\n",
    "TRAIN_PATH = 'food-images/food_c101_n10099_r64x64x3.h5'\n",
    "TEST_PATH = 'food-images/food_test_c101_n1000_r64x64x3.h5'\n",
    "\n",
    "IMAGE_CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 3, 3\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (10099, 64, 64, 3)  y_train shape:  (10099, 101)\n",
      "X_test shape:  (1000, 64, 64, 3)  y_test shape:  (1000, 101)\n"
     ]
    }
   ],
   "source": [
    "# GET DATA\n",
    "X_train = HDF5Matrix(TRAIN_PATH,\n",
    "                     'images')  # Shape (10099, 32, 32, 3) => [image_index][pixel_width_index][pixel_height_index][color_channel_index]\n",
    "y_train = HDF5Matrix(TRAIN_PATH, 'category')  # Shape (1000, 101) => [image_index][one_hot_encoded_vector]\n",
    "print(\"X_train shape: \", X_train.shape, \" y_train shape: \", y_train.shape)\n",
    "\n",
    "X_test = HDF5Matrix(TEST_PATH,\n",
    "                    'images')  # Shape (10099, 32, 32, 3) => [image_index][pixel_width_index][pixel_height_index][color_channel_index]\n",
    "y_test = HDF5Matrix(TEST_PATH, 'category')  # Shape (1000, 101) => [image_index][one_hot_encoded_vector]\n",
    "print(\"X_test shape: \", X_test.shape, \" y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Easy operations\n",
    "def weight_variable(shape):\n",
    "    \"\"\"\n",
    "    Initialise weight varaibles with smalls values around 0\n",
    "    \"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"\n",
    "    Generates bias of a given shape\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"\"\n",
    "     Downsamples the feature map by 2X\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "x = tf.placeholder(tf.float32, [None, 64, 64, 3]) # input of shape [picture_index][pixel_index]\n",
    "y_ = tf.placeholder(tf.float32, [None, 101]) # Correct answers [picture_index][answer_class_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "with tf.name_scope('conv1'):\n",
    "    # maps one 1 color image (3 channels) to 64 feature maps\n",
    "    W_conv1 = weight_variable([3, 3, IMAGE_CHANNELS, 64])\n",
    "    b_conv1 = bias_variable([64])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    \n",
    "tf.summary.tensor_summary('input', x)\n",
    "tf.summary.tensor_summary('W_conv1', W_conv1)\n",
    "    #tf.summary.tensor_summary('b_conv1', b_conv1)\n",
    "    #tf.summary.image('h_conv1', h_conv1)\n",
    "\n",
    "\n",
    "with tf.name_scope('pool1'):\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "with tf.name_scope('conv2'):\n",
    "    # maps 64 feature maps to 128 feature maps\n",
    "    W_conv2 = weight_variable([3, 3, 64, 128])\n",
    "    b_conv2 = bias_variable([128])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    \n",
    "tf.summary.tensor_summary('W_conv2', W_conv2)\n",
    "    #tf.summary.tensor_summary('b_conv2', b_conv2)\n",
    "    #tf.summary.image('h_conv2', h_conv2)\n",
    "\n",
    "\n",
    "with tf.name_scope('pool2'):\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "with tf.name_scope('conv3'):\n",
    "    # maps 128 feature maps to 256 feature maps\n",
    "    W_conv3 = weight_variable([3, 3, 128, 256])  # INPUT 3 channels => OUTPUT 64 CHANNELS\n",
    "    b_conv3 = bias_variable([256])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    \n",
    "tf.summary.tensor_summary('W_conv3', W_conv3)\n",
    "    #tf.summary.tensor_summary('b_conv3', b_conv3)\n",
    "    #tf.summary.image('h_conv3', h_conv3)\n",
    "\n",
    "\n",
    "with tf.name_scope('reshape'):\n",
    "    h_reshape = tf.reshape(h_conv3, [-1, 16 * 16 * 256])\n",
    "\n",
    "with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([16 * 16 * 256, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_fc1 = tf.nn.relu( tf.matmul(h_reshape, W_fc1) + b_fc1 )\n",
    "\n",
    "with tf.name_scope('fc2'):\n",
    "    W_fc2 = weight_variable([1024, 2048])\n",
    "    b_fc2 = bias_variable([2048])\n",
    "\n",
    "    h_fc2 = tf.nn.relu( tf.matmul(h_fc1, W_fc2) + b_fc2 )\n",
    "\n",
    "with tf.name_scope('readout'):\n",
    "    W_readout = weight_variable([2048, 101])\n",
    "    b_readout = bias_variable([101])  # 101 categories\n",
    "\n",
    "    y = tf.matmul(h_fc2, W_readout) + b_readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Cross entropy & stuff\n",
    "loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "optimizer_adam = tf.train.AdamOptimizer(1e-5).minimize(loss_cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.0\n",
      "\n",
      "step 1, test accuracy 0.011\n",
      "\n",
      "step 1, training accuracy 0.0\n",
      "\n",
      "step 2, training accuracy 0.03125\n",
      "\n",
      "step 3, training accuracy 0.0\n",
      "\n",
      "step 4, training accuracy 0.0\n",
      "\n",
      "step 5, training accuracy 0.0\n",
      "\n",
      "step 6, training accuracy 0.0\n",
      "\n",
      "step 7, training accuracy 0.0\n",
      "\n",
      "step 8, training accuracy 0.03125\n",
      "\n",
      "step 9, training accuracy 0.0\n",
      "\n",
      "step 10, training accuracy 0.0\n",
      "\n",
      "step 11, training accuracy 0.0\n",
      "\n",
      "step 12, training accuracy 0.03125\n",
      "\n",
      "step 13, training accuracy 0.03125\n",
      "\n",
      "step 14, training accuracy 0.0\n",
      "\n",
      "step 15, training accuracy 0.0\n",
      "\n",
      "step 16, training accuracy 0.0\n",
      "\n",
      "step 17, training accuracy 0.0\n",
      "\n",
      "step 18, training accuracy 0.0\n",
      "\n",
      "step 19, training accuracy 0.0\n",
      "\n",
      "step 20, training accuracy 0.0\n",
      "\n",
      "step 1, test accuracy 0.013\n",
      "\n",
      "step 21, training accuracy 0.0\n",
      "\n",
      "step 22, training accuracy 0.0\n",
      "\n",
      "step 23, training accuracy 0.0\n",
      "\n",
      "step 24, training accuracy 0.0\n",
      "\n",
      "step 25, training accuracy 0.0\n",
      "\n",
      "step 26, training accuracy 0.0\n",
      "\n",
      "step 27, training accuracy 0.03125\n",
      "\n",
      "step 28, training accuracy 0.03125\n",
      "\n",
      "step 29, training accuracy 0.0625\n",
      "\n",
      "step 30, training accuracy 0.0\n",
      "\n",
      "step 31, training accuracy 0.0\n",
      "\n",
      "step 32, training accuracy 0.0\n",
      "\n",
      "step 33, training accuracy 0.0\n",
      "\n",
      "step 34, training accuracy 0.03125\n",
      "\n",
      "step 35, training accuracy 0.0\n",
      "\n",
      "step 36, training accuracy 0.0\n",
      "\n",
      "step 37, training accuracy 0.03125\n",
      "\n",
      "step 38, training accuracy 0.0\n",
      "\n",
      "step 39, training accuracy 0.0\n",
      "\n",
      "step 40, training accuracy 0.0\n",
      "\n",
      "step 1, test accuracy 0.005\n",
      "\n",
      "step 41, training accuracy 0.03125\n",
      "\n",
      "step 42, training accuracy 0.03125\n",
      "\n",
      "step 43, training accuracy 0.0\n",
      "\n",
      "step 44, training accuracy 0.03125\n",
      "\n",
      "step 45, training accuracy 0.0\n",
      "\n",
      "step 46, training accuracy 0.03125\n",
      "\n",
      "step 47, training accuracy 0.0\n",
      "\n",
      "step 48, training accuracy 0.0625\n",
      "\n",
      "step 49, training accuracy 0.0\n",
      "\n",
      "step 50, training accuracy 0.0\n",
      "\n",
      "step 51, training accuracy 0.0\n",
      "\n",
      "step 52, training accuracy 0.0\n",
      "\n",
      "step 53, training accuracy 0.03125\n",
      "\n",
      "step 54, training accuracy 0.0\n",
      "\n",
      "step 55, training accuracy 0.0\n",
      "\n",
      "step 56, training accuracy 0.0\n",
      "\n",
      "step 57, training accuracy 0.0\n",
      "\n",
      "step 58, training accuracy 0.0\n",
      "\n",
      "step 59, training accuracy 0.0\n",
      "\n",
      "step 60, training accuracy 0.0\n",
      "\n",
      "step 1, test accuracy 0.012\n",
      "\n",
      "step 61, training accuracy 0.03125\n",
      "\n",
      "step 62, training accuracy 0.09375\n",
      "\n",
      "step 63, training accuracy 0.0\n",
      "\n",
      "step 64, training accuracy 0.0\n",
      "\n",
      "step 65, training accuracy 0.0\n",
      "\n",
      "step 66, training accuracy 0.03125\n",
      "\n",
      "step 67, training accuracy 0.03125\n",
      "\n",
      "step 68, training accuracy 0.0\n",
      "\n",
      "step 69, training accuracy 0.0\n",
      "\n",
      "step 70, training accuracy 0.0625\n",
      "\n",
      "step 71, training accuracy 0.0\n",
      "\n",
      "step 72, training accuracy 0.03125\n",
      "\n",
      "step 73, training accuracy 0.03125\n",
      "\n",
      "step 74, training accuracy 0.0\n",
      "\n",
      "step 75, training accuracy 0.0\n",
      "\n",
      "step 76, training accuracy 0.0\n",
      "\n",
      "step 77, training accuracy 0.0\n",
      "\n",
      "step 78, training accuracy 0.03125\n",
      "\n",
      "step 79, training accuracy 0.0\n",
      "\n",
      "step 80, training accuracy 0.0\n",
      "\n",
      "step 1, test accuracy 0.01\n",
      "\n",
      "step 81, training accuracy 0.0\n",
      "\n",
      "step 82, training accuracy 0.03125\n",
      "\n",
      "step 83, training accuracy 0.03125\n",
      "\n",
      "step 84, training accuracy 0.0\n",
      "\n",
      "step 85, training accuracy 0.0\n",
      "\n",
      "step 86, training accuracy 0.0\n",
      "\n",
      "step 87, training accuracy 0.0\n",
      "\n",
      "step 88, training accuracy 0.0\n",
      "\n",
      "step 89, training accuracy 0.0\n",
      "\n",
      "step 90, training accuracy 0.0\n",
      "\n",
      "step 91, training accuracy 0.0\n",
      "\n",
      "step 92, training accuracy 0.0\n",
      "\n",
      "step 93, training accuracy 0.03125\n",
      "\n",
      "step 94, training accuracy 0.0\n",
      "\n",
      "step 95, training accuracy 0.0\n",
      "\n",
      "step 96, training accuracy 0.0\n",
      "\n",
      "step 97, training accuracy 0.03125\n",
      "\n",
      "step 98, training accuracy 0.03125\n",
      "\n",
      "step 99, training accuracy 0.0\n",
      "\n",
      "Final test accuracy 0.012\n"
     ]
    }
   ],
   "source": [
    "#Execute the session\n",
    "merged = tf.summary.merge_all()\n",
    "log_writer = tf.summary.FileWriter('tensorboard-v2', sess.graph)\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    #Choose randomly a batch of data\n",
    "    int_start = random.randint(0, X_train.shape[0] - BATCH_SIZE )\n",
    "    int_end = int_start + BATCH_SIZE\n",
    "    batch = ( X_train[ int_start:int_end ], y_train[ int_start:int_end ] )\n",
    "\n",
    "    #Run the training\n",
    "    optimizer_adam.run(session=sess, feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "        \n",
    "    #Find and print the accuracy\n",
    "    #train_accuracy, summary = sess.run( [accuracy, merged], feed_dict={x: batch[0], y_: batch[1]} )\n",
    "    #sess.run(merged,feed_dict=feed_dict(False))\n",
    "    summary, train_accuracy = sess.run([merged, accuracy], feed_dict={x: batch[0], y_: batch[1]})\n",
    "    #train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    print('step %d, training accuracy %s\\n' % (i, train_accuracy))\n",
    "\n",
    "    #Evalaute the accuracy on some training set evry x epochs\n",
    "    if i % 20 == 0:\n",
    "        test_accuracy = accuracy.eval(feed_dict={x: X_test, y_: y_test})\n",
    "        print('step %d, test accuracy %s\\n' % (1, test_accuracy))\n",
    "        \n",
    "        log_writer.add_summary(summary, i)\n",
    "\n",
    "\n",
    "#Final Accuracy\n",
    "test_accuracy = accuracy.eval(feed_dict={x: X_test, y_: y_test})\n",
    "print('Final test accuracy %g' % (test_accuracy))\n",
    "log_writer.flush()\n",
    "log_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
